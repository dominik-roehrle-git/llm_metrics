{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Evaluation\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "- To evaluate all metrics specify the path where the model has generated the text and specify the path where you want to store the metrics\n",
    "- Important: The paths in the folder ./evaluation already have the metrics with the .csv files included. To make a new evaluation, delete the files with the metrics first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_path = \"\"\n",
    "# for example: \"./evaluation_files/variation3_llama2_chat/prompt_size8/generation/\"\n",
    "metrics_path = \"\"\n",
    "# for example: \"./evaluation_files/variation3_llama2_chat/prompt_size8/metrics/\"\n",
    "\n",
    "for filename in os.listdir(generation_path):\n",
    "    evaluate = Evaluation(evaluation_file=os.path.join(generation_path, filename), metrics_folder=metrics_path)\n",
    "    evaluate.evaluate_all_metrics()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to convert the results to a latex table (optional, but saves a lot of time)\n",
    "def convert_to_latex(data_array, caption=\"Your Table Caption Here\", label=\"your-label\"):\n",
    "    table_data = {'supports': {}, 'refutes': {}}\n",
    "    for item in data_array:\n",
    "        label, hops, _ = item['filename'].split('_')\n",
    "        label = label.lower()\n",
    "        table_data[label][hops] = item['mean']\n",
    "\n",
    "    latex_table = \"\\\\begin{table}[h]\\n\\\\centering\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{lccc}\\n\"\n",
    "    latex_table += \" & Hops 2 & Hops 3 & Hops 4 \\\\\\\\\\\\hline\\n\"\n",
    "    for label in ['supports', 'refutes']:\n",
    "        latex_table += f\"{label.capitalize()} \"\n",
    "        for hops in ['hops2', 'hops3', 'hops4']:\n",
    "            latex_table += f\"& {table_data[label][hops]:.2f} \"\n",
    "        latex_table += \"\\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    latex_table += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    latex_table += f\"\\\\label{{{label}}}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "\n",
    "    # Printing the LaTeX table\n",
    "    print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2 Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/basic_llama2/metrics/\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2-chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/basic_llama2_chat/metrics/\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation1_llama2_chat/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation2_llama2_chat/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation3_llama2_chat/prompt_size2/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation3_llama2_chat/prompt_size4/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation3_llama2_chat/prompt_size6/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./evaluation_files/variation3_llama2_chat/prompt_size8/metrics\"\n",
    "bart_mean_scores = []\n",
    "bleu_mean_scores = []\n",
    "bert_mean_scores = []\n",
    "rouge_mean_scores = []\n",
    "\n",
    "for filename in os.listdir(metrics_path):\n",
    "    df = pd.read_csv(os.path.join(metrics_path, filename))\n",
    "    if filename.endswith(\"BART.csv\"):\n",
    "        bart_mean_scores.append({\"filename\": filename, \"mean\" : df['bartscore'].mean()})\n",
    "    elif filename.endswith(\"BLEU.csv\"):\n",
    "        bleu_mean_scores.append({\"filename\": filename, \"mean\" : df['bleu_score'].mean()})\n",
    "    elif filename.endswith(\"BERT.csv\"):\n",
    "        bert_mean_scores.append({\"filename\": filename, \"mean\" : df['F1 Score'].mean()})\n",
    "    elif filename.endswith(\"ROUGE.csv\"):\n",
    "        rouge_mean_scores.append({\"filename\": filename, \"mean\" : df['ROUGE-1 F'].mean()})\n",
    "        \n",
    "        \n",
    "print(\"Bart: \")\n",
    "for obj in bart_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bert: \")\n",
    "for obj in bert_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Bleu: \")\n",
    "for obj in bleu_mean_scores:\n",
    "    print(obj)\n",
    "\n",
    "#print(\"Rouge: \")\n",
    "#for obj in rouge_mean_scores:\n",
    "#    print(obj)\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(\"Bart: \")\n",
    "print(sum([obj['mean'] for obj in bart_mean_scores]) / len([obj['mean'] for obj in bart_mean_scores]))\n",
    "print(\"Bert: \")\n",
    "print(sum([obj['mean'] for obj in bert_mean_scores]) / len([obj['mean'] for obj in bert_mean_scores]))\n",
    "print(\"Bleu: \")\n",
    "print(sum([obj['mean'] for obj in bleu_mean_scores]) / len([obj['mean'] for obj in bleu_mean_scores]))\n",
    "#print(\"Rouge:\")\n",
    "#print(sum([obj['mean'] for obj in rouge_mean_scores]) / len([obj['mean'] for obj in rouge_mean_scores]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
